Model Quantization : 
	
	https://huggingface.co/docs/transformers/quantization
	https://huggingface.co/docs/transformers/main/en/quantization
	https://huggingface.co/blog/hf-bitsandbytes-integration
	
	https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/
	https://deci.ai/quantization-and-quantization-aware-training/
	
	https://medium.com/@syedhamzatahir1001/why-quantization-fails-for-large-lms-and-its-soultion-bb5cf24b8203
	https://medium.com/@sachinsoni600517/introduction-to-model-quantization-4effc7a17000
	https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be
	
	https://maartengrootendorst.substack.com/p/which-quantization-method-is-right
	https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing
	
	https://www.tensorops.ai/post/what-are-quantized-llms	
	https://www.youtube.com/watch?v=v1oHf1KV6kM [codebasics tensorflow quantization]
	
	
	what is quantization ?
	
		converting the model params dtype from one to another
		
	why is it required?
	
		To fit large models into lesser memory and to improve the inference speed, without much reduction in model accuracy.
		
	Types : 
		
		Post Training Quantization [PTQ]
			
			Load Pre-Trained LLM -> Fine Tune (if needed) -> QUANTIZE
			
			Pros : Model Size reduction, Increase in inference speed
			Cons : Decrease in Model Accuracy.
		
		Quantization Aware Training [QAT]
		
			Load Pre-Trained LLM -> QUANTIZE -> Fine Tune -> QUANTIZE (if needed)
			
			Pros : Model Size reduction, Increase in inference speed
			Cons : Decrease in Model Accuracy, but performs better than PTQ [as its being finetuned]
		
	what are the strategies available?

	[8 bit quantization] :::

		https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html 
			- Absolute Max Quantization 
		
				## Can handle Symmetric distributions
				
				X is a vector of x i.e X = [-3.0 .....0.1 ..... 3.2]
				
				scale = (127 / max(X))
			
				x quant = ROUND(x * scale)
				
				x dequant = (x quant / scale)
				
				e.g. 
				
				x = 0.1; max(X) = 3.2
				
				scale = (127/3.2) = 39.6875
				
				x quant = (0.1 * 39.6875) = 4
				
				x dequant = (4 / 39.6875) = 0.1008
				
				error = 0.1008 - 0.1000 = 0.0008
			
			
			- Zero Point Quantization
		
				## Can handle Asymmetric distributions
			
				X is a vector of x i.e X = [-3.0 .....0.1 ..... 3.2]
				
				
				scale =  (255 / (max(X) - min(X)))
				
				zeropoint = (- round (scale * min(X)) - 128
				
				
				x quant = ROUND(( x * scale) + zeropoint)
				
				x dequant = ((x quant - zeropoint) / scale) 
				
				e.g. 
				
				x = 0.1; max(X) = 3.2; min(X) = -3.0
				
				scale = (255 / (3.2 - (-3.0))) = (255/6.2) = 41.12903226
				
				zeropoint = (- round (41.12903226 * (-3.0)) - 128 = 123 - 128 = -5
				
				x quant = ROUND((0.1 * 41.12903226) + (-5)) = -1
				
				x dequant = ((-1 - (-5))/(41.12903226)) = 4/(41.12903226) = 0.097254902
				
				error = 0.0972 - 0.1000 = 0.0028

				NOTE : Better than Absolute Max Quantization in general
			
		
			SPECIAL NOTE : 
		
				1. we could apply above approaches on varioud ways : 
			
					- entire model in one pass
					- on each layers 
					- on each layers per tensor
					- on each weight
					
					
					general prefrence approach is vector wise quantization on each layer.
					
				2. Major disadvantage in the above both approaches, unabel to handle vectors with outlier weights.
		
			- Mixed Precision (LLM.int8()) quantization
		
			
				- relies on absolute max quantization for non-outliers and introduces mixed precision quantization.
				
				- How it works?
					- Choose a threshold for outlier
					- Based on the threshold, identify the outliers from the input vector and seperate them and retain in FP16 format.
					
					- For non-outliers, Apply row wise absolute max quantization on hidden states, column wise absolute max quantization on weights, and the resultant vector will be in INT8 format.
					- Now, DEquantize the results back to FP16 format.
					- Add the dequantized results with outliers in FP16 format.
					
					- resultant is the quantised results for this approach.
					
				- why are we doing so?

					- as quantizing outliers , results in huge errors and that will propogate/amplify to other layers and result in huge loss.
					- but will this reduce the model size, if we skip the outliers?
						yes, as outliers account for 1% of total values of layer weights. [study results]
	
	[4 bit quantization] :::
		
		https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html  [GPTQ]
		https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html [GGML in GGUF format]
		https://huggingface.co/blog/4bit-transformers-bitsandbytes [NF4/FP4]
		https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html [EXL2]
		https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8 [AWQ]
		
		
		- GPTQ 
		
			- Usage Guide : [https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization]
		
			- GPTQ follows OBQ [Optimal Brain Quantization] framework
			
			- what OBQ framework is ?
				
				- order the weights based on, which will add the least additional error.
				- pick up the first one and quantize it
				- calculate the loss and compensate by adjusting other non-quantised weights.
				- Repeat then.
				- Problem here are the outliers, bcoz it can lead to high quantization error.
				- So, OBQ applied a simple technique, quantize outliers as soon as they appear.
				- And then, this process could be computationally heavy.
				- So, OBQ applied a simple techiqnue, as soon as a weight is quantized, 
					adjust the Hessian matrix(H) by simply removing the ROW and COLUMN where the quantized weight was present, 
					to avoid re-computations.
				- employed vectorization to process multiple rows at a time.
				
			- what alternatives GPTQ done to OBQ framework?
			
				- OBTQ : quantize weights in the order, which will add the least error.
				- GPTQ : quantize all weights in the same order for all rows of the matrix, 
						 Reason : as order doesnt matter much due weights are goanna get quantized evenetually.
						 
				- GPTQ : employed lazy batch updates [compute batch of columns at a time]
				
				- GPTQ : cholesky decomposition technique to solve numerical errors.
				
			- Entire algo can be summarized as follows :
			
				1.The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)
				2.It then runs in loops, handling batches of columns at a time.
				3.For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.
				4.After processing the batch, it updates all remaining weights in other blocks based on that block’s errors.
				
			- Library : AutoGPTQ, HF Optimum
		
		- GGML 
		
			Variants Explanation : 
				https://www.reddit.com/r/LocalLLaMA/comments/14gjz8h/i_have_multiple_doubts_about_kquant_models_and/
				https://huggingface.co/TheBloke/koala-7B-GGML
			
			Usage Guide: 
			
				Load Pre-Trained model in other framework -> Convert to GGUF format -> Quantize then -> Inference
				
				https://github.com/ggerganov/llama.cpp/discussions/2948
			
			- what is GGML : 
				GG - owner name  Georgi Gerganov ML models written in c.
				llama.cpp - owned by GG - written in cpp.
			
			- GGML models in GGUF format [for scalability] or the previous formats.
			
			- How it quantize?
				- Basically, it groups blocks of values and rounds them to lower precision.
				- it keeps certain layers in higher precision.
				- this mixed precision approach increases accuracy and lowers resource usage.
				
			- Main Difference between GPTQ and GGML 
				- you can run GGML in a CPU.
				- however, you can offload certain layers also in GPU, with llama.cpp
				
				
		
		- FP4/NF4
		
			FP4 - Floating Point - 4 bit
			NF4 - Normalized float - 4 bit
		
			Usage Guide : 
			
			https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing [just for inference]
			https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing [for training and inference]
			
			Reference :
			
			https://id2thomas.medium.com/ml-bitsandbytes-nf4-quantize-dequantize-analysis-1ad91d9912c9		

			Quantizes tensor A by dividing it into blocks of 4096 values.
			Then the absolute maximum value within these blocks is calculated
			for the non-linear quantization.
			
			same as LLM.int8(), except that quantisation is for 4 bit.
		
		- EXL2
		
			Under the hood, ExLlamaV2 leverages the GPTQ algorithm to lower the precision of the weights while minimizing the impact on the ouput. 
			
			So why are we using the “EXL2” format instead of the regular GPTQ format? EXL2 comes with a few new features:

				It supports different levels of quantization: it’s not restricted to 4-bit precision and can handle 2, 3, 4, 5, 6, and 8-bit quantization.
				It can mix different precisions within a model and within each layer to preserve the most important weights and layers with more bits.
				
			ExLlamaV2 uses this additional flexibility during quantization. 
			It tries different quantization parameters and measures the error they introduce. 
			On top of trying to minimize the error, ExLlamaV2 also has to achieve the target average number of bits per weight given as an argument. 
			Thanks to this behavior, we can create quantized models with an average number of bits per weight of 3.5 or 4.5 for example.


		
		- AWQ 
		
			[https://www.reddit.com/r/LocalLLaMA/comments/13yehfn/new_quantization_method_awq_outperforms_gptq_in/]
		
			The paper proposes Activation-aware Weight Quantization (AWQ), 
			a post-training quantization method for compressing and accelerating large language models (LLMs).

			AWQ is based on the observation that not all weights in LLMs are equally important. 
			Only a small fraction (around 1%) of "salient" weights contribute significantly to performance.

			AWQ protects important weights by performing per-channel scaling instead of keeping them in full precision. 
			This avoids the hardware inefficiency of mixed-precision formats.

			The scaling factors are determined based on the activation distribution, not the weight distribution. 
			Weights with larger activation magnitudes are found to be more important.

			AWQ does not rely on reconstructing weights through backpropagation or regression, which can cause overfitting to the calibration set. 
			This helps preserve the generalizability of LLMs.

			AWQ also does not require data layout reordering, maintaining hardware efficiency. 
			The paper shows AWQ achieves 1.45x speedup over GPTQ and is 1.85x faster than cuBLAS FP16 implementation.

			Experiments show AWQ outperforms round-to-nearest quantization and GPTQ on various LLMs and tasks,
			including instruction-tuned models and multi-modal language models.

			The paper proposes efficient quantized GPU kernels that perform weight dequantization without data reordering, 
			further improving efficiency.