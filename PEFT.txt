Parameter Efficient Fine Tuning :

AsFarAsIKnow, BERT doesn't make use of gradual unfreezing. 
Instead, during fine-tuning all model parameters are trainable. 
It can result in catastrophic forgetting, if you train it for long enough/ large enough learning rate,
which is why we usually fine tune for 1-2 epochs at a low learning rate.
When it comes to doing it yourself, you'll should be able to just tweak the number of epochs/train steps 
and then find which number gives you the best results. 
InMyOpinion, anymore than a couple epochs will result in overfitting/forgetting.

Hope that helps.

https://vinija.ai/nlp/parameter-efficient-fine-tuning/ [good read with all methods; no code]
https://huggingface.co/blog/peft [good read but in brief; with code]
https://github.com/huggingface/peft
https://github.com/huggingface/peft/tree/main/examples/sequence_classification [code for all methods]
https://www.leewayhertz.com/parameter-efficient-fine-tuning/ [good read with all methods; no code]
https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning [only for LoRA with code]
https://medium.com/@rtales/parameter-efficient-fine-tuning-peft-a-novel-approach-for-fine-tuning-large-language-models-ed9d2bf4acf3 [discussed all methods in brief]
https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-1-571a472612c4 [good read for LoRA - detailed]
https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f [good read for QLoRA - detailed]
https://markovate.com/blog/parameter-efficient-fine-tuning-peft-of-llms-a-practical-guide/ [good read with all methods; no code]
https://blog.searce.com/a-guide-to-parameter-efficient-fine-tuning-llm-models-3222d072e701 [only for LoRA with code]
https://www.linkedin.com/pulse/parameter-efficient-fine-tuning-large-language-models-pankaj-a/ [only for LoRA with code]
https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/ [only for LoRA + QLoRA with code]

https://harininarasimhan.medium.com/part-1-fine-tuning-using-huggingface-transformers-e3e8de75d748 [fine tuning]
https://harininarasimhan.medium.com/part-2-overcoming-challenges-with-parameter-efficient-training-25e3f7147cd5 [PEFT - LoRA and QLoRA]

https://medium.com/@ud.chandra/instruction-fine-tuning-llama-2-with-pefts-qlora-method-d6a801ebb19 [Instruction Tuning]

https://stackoverflow.com/questions/76451205/difference-between-instruction-tuning-vs-non-instruction-tuning-large-language-m [instruction vs fine tuning]
https://newsletter.ruder.io/p/instruction-tuning-vol-1 [instruction vs fine tuning - detailed]

https://heidloff.net/article/introduction-to-prompt-tuning/
https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting
https://huggingface.co/spaces/PEFT/causal-language-modeling/blob/main/prefix-tuning-clm.ipynb
https://github.com/huggingface/notebooks/blob/main/peft_docs/en/clm-prompt-tuning.ipynb
https://colab.research.google.com/github/huggingface/notebooks/blob/main/peft_docs/en/clm-prompt-tuning.ipynb
https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods?configurations=p-tuning

https://rentry.org/llm-training
https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Multitask_Prompt_and_PTuning.ipynb


https://blog.wordbot.io/ai-artificial-intelligence/prompt-tuning-vs-prefix-tuning-understanding-the-differences-in-nlp-techniques/ [perfect one]

https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting#soft-prompts [perfect one]
https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter
https://huggingface.co/docs/peft/main/en/conceptual_guides/ia3


https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora

	Prompt Methods 
	
		Prompt Tuning
		
			Prompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. 
			This way, you can use one pretrained model whose weights are frozen, 
			and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate model. 
			As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.
		
		Prefix Tuning
		
			Prefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or prefix. 
			Only the prefix parameters are optimized and added to the hidden states in every layer of the model. 
			The tokens of the input sequence can still attend to the prefix as virtual tokens. 
			As a result, prefix tuning stores 1000x fewer parameters than a fully finetuned model, 
			which means you can use one large language model for many tasks
		
		P Tuning
		
			P-tuning is designed for natural language understanding (NLU) tasks and all language models. 
			It is another variation of a soft prompt method; 
			P-tuning also adds a trainable embedding tensor that can be optimized to find better prompts, 
			and it uses a prompt encoder (a bidirectional long-short term memory network or LSTM) to optimize the prompt parameters. 
			
			Unlike prefix tuning though:

				the prompt tokens can be inserted anywhere in the input sequence, and it isnâ€™t restricted to only the beginning
				the prompt tokens are only added to the input instead of adding them to every layer of the model
				introducing anchor tokens can improve performance because they indicate characteristics of a component in the input sequence
			
			The results suggest that P-tuning is more efficient than manually crafting prompts, 
			and it enables GPT-like models to compete with BERT-like models on NLU tasks
		
		Multitask Prompt Tuning
		
			Multitask prompt tuning (MPT) learns a single prompt from data for multiple task types that can be shared for different target tasks. 
			Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. 
			
			MPT consists of two stages:

				source training - for each task, its soft prompt is decomposed into task-specific vectors. 
								  The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
				target adaptation - to adapt the single prompt for a target task, a target prompt is initialized 
									and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
		
	Adapters
	
	
		- Adding Extra layers 
		
	Re-Parameterisation 
	
		- Decomposing Weight matrixes
			- LoRA
				- Decomposing weight matrix into 2 smaller matrices using low rank decomposition rank r.
				- original matrix will be frozen
				- new matrices will be updated during training
				- later, original and new will be added
				
				- It can be applied to any weights but attention layers are focussed in HF implementation.
				
				W = p * q
				
				A = p * r
				
				B = r * q
				
				result = A * B
				
			- LoHa
			
				- similar to LoRA, decomposes weight matrxi into 4, and uses hadamard product (i.e. element wise product) instead of matrix product.
				
				W = p * q
				
				A1 = p * r
				
				B1 = r * q
				
				interim1 = A1 * B1
				
				A2 = p * r
				
				B2 = r * q
				
				interim2 = A2 * B2
				
				result = interim1 (hadamard product) interim2
				
				
			- LoKr
			
				- similar to LoRA and LoHa
				- creates a block matrix which preserves the original matrix weights
				
				- decomposes weight matrix into 2, and uses kronecker product (i.e. element wise product) instead of matrix product.
				
				W = p * q
				
				A = v(p) * r
				
				B = r * v(q)
				
				C = u(p) * u(q)
				
				interim = (A * B)
				result = C (kronecker product) interim
			
		- rescaling vectors
			- IA3
				- IA3 weights are learned during fine tuning and added to the outputs of key and value layers, 
				and to the input of the second feedforward layer in each transformer block.
				
				- It can be applied to any as well, but above layers are focussed in HF implementation.
		
		- Other Variants
			- AdaLoRA
			- OFT
			- Llama-Adapter
			
		
		