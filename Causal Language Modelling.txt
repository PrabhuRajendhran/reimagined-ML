Causal Language Modelling :  

		https://huggingface.co/docs/transformers/tasks/language_modeling
		https://huggingface.co/docs/transformers/generation_strategies
		https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt
		https://huggingface.co/blog/how-to-generate
		https://huggingface.co/blog/constrained-beam-search
	
	
Step 1 : Gather Data

Step 2 : Filter the data, if needed

Step 3 : Tokenize and PreProcess Data
			- Tokenize 
			- Truncate and/or return overflowing tokens
			- you can throw out the last chunk if not equal to context length or pad in next step
			
Step 4.1 : Pad and Collate the batched data, if last chunck (with lesser context length) is not thrown out.
Step 4.2 : datacollatorforlanguagemodelling, automatically create labels (use param mlm = true for masked lm, mlm = false for causual lm)
		   

Step 5 : Instantiate a new model using the respective model config.

Step 6 : Define Training Args [learning params, eval strategy, compute metrics function]

Step 7 : Insantiate Trainer

Step 8 : Start Training

Step 9 : Evaluate and Further, Tune/Train or Finalize model 
			
			
		Metric : Perplexity
