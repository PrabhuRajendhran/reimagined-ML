Causal Language Modelling :  
	
	
Step 1 : Gather Data

Step 2 : Prepare Data with columns : Input Text, Translated Text

Step 3 : Split into Train and Test Data [and validation data if needed] - now or after pre-processing

Step 4 : Tokenize and PreProcess Data
			- Tokenize [make sure you are passing output text to "text_targets" parameter of tokenizer.
			- Truncate 
			
Step 5 : Pad and Collate the batched data 

Step 6 : Define Training Args [learning params, eval strategy, compute metrics function]

Step 7 : Insantiate Trainer

Step 8 : Start Training

Step 9 : Evaluate and Further, Tune or Finalize model 
			[NOTE : not advisable to fine-tune the model for not more than 2-3 epochs, as it may lead to "Catatrophic Forgetting"]
			
		Metric : BLEU, SacreBLEU
