Supervised :

Step 1 : Gather Data

Step 2 : Label Data

Step 3 : Instantiate a model

Step 4 : Fit model to the labelled data

Step 5 : Train and Tune the model

Step 6 : Evaluate model on unseen data

Step 7 : Finalize and save model



UnSupervised : 

Step 1 : Gather Data

Step 2 : No need to Label Data

Step 3 : Instantiate a model

Step 4 : Fit model to the Unlabelled data

Step 5 : Train and Tune the model

Step 6 : Evaluate model on unseen data [we could use labelled data as well here]

Step 7 : Finalize and save model


Semi Supervised : https://www.v7labs.com/blog/semi-supervised-learning-guide
				  https://spotintelligence.com/2023/12/28/semi-supervised-machine-learning-made-simple-5-algorithms-how-to-python-tutorial/
				  
Step 1 : Gather Data

Step 2 : Label limited Data and dont label the rest

Step 3 : Instantiate a model

Step 4 : Fit model to the labelled (limited) data

Step 5 : Forms a loop

Step 5.1 : Train and Tune the model

Step 5.2 : Predict on the unlabelled data using the trained model

Step 5.3 : Get High Confidence labels and add those to the labelled dataset.

Step 5.4 : Re-Train with additional data in step #5.3 and Tune the model

Step 6 : Evaluate model on unseen data

Step 7 : Finalize and save model


Zero Shot Learning :
			https://huggingface.co/learn/nlp-course/en/chapter1/4?fw=pt#transfer-learning

			https://aws.amazon.com/blogs/machine-learning/zero-shot-text-classification-with-amazon-sagemaker-jumpstart/

			https://joeddav.github.io/blog/2020/05/29/ZSL.html

			https://huggingface.co/tasks/zero-shot-classification

			https://spotintelligence.com/2023/08/01/zero-shot-classification/


Step 1 : Gather Data

Step 2 : No need to Label Data

Step 3 : Load a Pre-Trained model

Step 4 : Predict on the unlabelled data using the loaded pre-trained model

Step 5 : How prediction works? 
		
		
		Option 1a : Latent Embedding Approach  (without projection)
			1. Model Projects inout sentences and candidate labels into latent embedding space.
			2. Calculate how similar each candidate label against all sentences.
			3. Return the more similar label for each sentence.
		
		Option 1b : Latent Embedding Approach  (with projection)
			1.1. Model Projects inout sentences and candidate labels into latent embedding space.
			1.2. Additional Transformation matrix is used to better learn the representation from individual candaite labels to pooled vectors.
					1.2.1. Take the top K most frequent words V in the vocabulary of a word2vec model
					1.2.2. Obtain embeddings for each word using word2vec,Φword(V)
					1.2.3. Obtain embeddings for each word using S-BERT, Φsent(V)
					1.2.4. Learn a least-squares linear projection matrix Z with L2 regularization from Φ sent (V) to Φword(V)
					
					Since we have only learned this projection for embeddings of single words,
					we cannot expect it to learn an effective mapping between S-BERT sequence representations and labels embedded with word2vec.
					Instead, we use Z in our classification only as an additional transformation to S-BERT embeddings for both sequences and labels

					Model (from) : c^ = argmax​ cos(Φ sent (x),Φ sent (c)) [for all class c∈C
					Model (to) : c^ = argmax​ cos(Φ sent (x) * Z,Φ sent (c) * Z) [for all class c∈C

			2. Calculate how similar each candidate label against all sentences.
			3. Return the more similar label for each sentence.
		
		
		Option 2 : Natural Language Inference Approach 
			1. Construct the "hypothesis" using the labels provided by users.
			2. Consider the input sentence as "premise".
			
			How hypothesis is constructed?
			
			Input to the model is constructed as Premise/Hypothesis pair 
			such that Premise is the Text itself & Hypothesis has a template of “this example is {}” for each Candidate Label
			
			3. Model tells whether the premise entails/contradicts the hypothesis.
				How the model tells? 
					Model projects both Premise and Hypothesis into the same latent space based on its knowledge gained during pre-training.
					Calculate where both premise and hypothesis lies in that space i.e. how similar. 
					Based on that, it returns the response.
					
					
		Option 3 : Approaching the classification as a Cloze Task
		
			1. Need User to construct Cloze template. [Masked sentence that could fit the label]
			2. Ask LM to fill the blank given the sentence.
		
		
		Option 4 : LM To question answer
		
			1. LM to answer which class, the sentence belongs to?
		
					
Few Shot Learning : 
			https://huggingface.co/docs/setfit/conceptual_guides/setfit
			https://huggingface.co/blog/setfit
			https://www.width.ai/post/what-is-setfit
			https://www.v7labs.com/blog/contrastive-learning-guide
			https://spotintelligence.com/2023/06/29/few-shot-learning/
			https://joeddav.github.io/blog/2020/05/29/ZSL.html


SetFit (Sentence Transformers Fine Tuning) Training

Part 1 : Embedding Fine Tuning Phase

However, models that are good at Semantic Textual Similarity (STS) are not necessarily immediately good at our classification task. 
For example, according to an embedding model, the sentence of 1) "He biked to work." will be much more similar to 2) "He drove his car to work." than to 3) "Peter decided to take the bicycle to the beach party!". 
But if our classification task involves classifying texts into transportation modes, then we want our embedding model to place sentences 1 and 3 closely together, and 2 further away.
To do so, we can finetune the chosen sentence transformer embedding model. 
The goal here is to nudge the model to use its pretrained knowledge in a different way that better aligns with our classification task, 
rather than making the completely forget what it has learned.
For finetuning, SetFit uses contrastive learning. 
This training approach involves creating positive and negative pairs of sentences. 
A sentence pair will be positive if both of the sentences are of the same class, and negative otherwise. 
For example, in the case of binary “positive”-“negative” sentiment analysis, ("The movie was awesome", "I loved it") is a positive pair, 
and ("The movie was awesome", "It was quite disappointing") is a negative pair.
During training, the embedding model receives these pairs, and will convert the sentences to embeddings. 
If the pair is positive, then it will pull on the model weights such that the text embeddings will be more similar, and vice versa for a negative pair. 
Through this approach, sentences with the same label will be embedded more similarly, and sentences with different labels less similarly.
Conveniently, this contrastive learning works with pairs rather than individual samples, and we can create plenty of unique pairs from just a few samples. 
For example, given 8 positive sentences and 8 negative sentences, we can create 28 positive pairs and 64 negative pairs for 92 unique training pairs. 
This grows exponentially to the number of sentences and classes, and that is why SetFit can train with just a few examples 
and still correctly finetune the sentence transformer embedding model. However, we should still be wary of overfitting.


Part 2 : Classifier Training Phase

Once the sentence transformer embedding model has been finetuned for our task at hand, we can start training the classifier.
This phase has one primary goal: create a good mapping from the sentence transformer embeddings to the classes.
Unlike with the first phase, training the classifier is done from scratch and using the labeled samples directly, rather than using pairs. 
By default, the classifier is a simple logistic regression classifier from scikit-learn. 
First, all training sentences are fed through the now-finetuned sentence transformer embedding model, 
and then the sentence embeddings and labels are used to fit the logistic regression classifier. 
The result is a strong and efficient classifier.

Using these two parts, SetFit models are efficient, performant and easy to train, even on CPU-only devices.


Step 1 : Gather Data

Step 2 : Label (limited - 8 to 16 examples per class) Data

Step 3 : Load a Pre-Trained Sentence Tranformers model

Step 4 : FineTune the loaded model on the (limited) labelled data 

		1. Generate Sentence pairs for the limited data using contrastive learning [1 in class and 1 out class per example]
		2. Fine Tune on the generated data, by moving embeddings of each sentence in a positive pair closer and in a negative pair farther.
		3. Encode those pairs with finetuned ST and generate embeddings.

Step 5 : Train the Classfication head with the embeddings data.

Step 6 : Predict on the unseen data.








Contrastive Learning : https://www.v7labs.com/blog/contrastive-learning-guide

Contrastive Learning is a Machine Learning paradigm where unlabeled data points are juxtaposed[placing - forming pairs] against each other 
to teach a model which points are similar and which are different.
That is, as the name suggests, samples are contrasted against each other, 
and those belonging to the same distribution are pushed towards each other in the embedding space. 
In contrast, those belonging to different distributions are pulled against each other.



Meta Learning : https://spotintelligence.com/2023/08/10/understanding-meta-learning/



Active Learning : https://spotintelligence.com/2023/08/08/active-learning-in-machine-learning/



Self Supervised : https://www.v7labs.com/blog/self-supervised-learning-guide