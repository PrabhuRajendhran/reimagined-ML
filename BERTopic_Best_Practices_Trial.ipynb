{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNWv-K-xiZsA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install bertopic\n",
        "!pip install datasets\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\n",
        "\n",
        "# Extract abstracts to train on and corresponding titles\n",
        "abstracts = dataset[\"abstract\"]\n",
        "titles = dataset[\"title\"]"
      ],
      "metadata": {
        "id": "TE67L83cuLzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts[0]"
      ],
      "metadata": {
        "id": "R_BoAGzxwNHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "JTwXxSnPupbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
      ],
      "metadata": {
        "id": "MbI9VptGxop4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ],
      "metadata": {
        "id": "Swmkcx5S3e9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))"
      ],
      "metadata": {
        "id": "xyIFi06Vzeg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
        "\n",
        "# KeyBERT\n",
        "keybert_model = KeyBERTInspired()\n",
        "\n",
        "# Part-of-Speech\n",
        "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
        "\n",
        "# MMR\n",
        "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "# GPT-3.5\n",
        "prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "The topic is described by the following keywords: [KEYWORDS]\n",
        "\n",
        "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
        "topic: <topic label>\n",
        "\"\"\"\n",
        "client = openai.OpenAI(api_key=\"sk-...\")\n",
        "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
        "\n",
        "# All representation models\n",
        "representation_model = {\n",
        "    \"KeyBERT\": keybert_model,\n",
        "    # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
        "    \"MMR\": mmr_model,\n",
        "    \"POS\": pos_model\n",
        "}"
      ],
      "metadata": {
        "id": "uaxb00nfzejc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  vectorizer_model=vectorizer_model,\n",
        "  representation_model=representation_model,\n",
        "\n",
        "  # Hyperparameters\n",
        "  top_n_words=10,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(abstracts, embeddings)"
      ],
      "metadata": {
        "id": "i3K4ehYozTBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "w3WRXoRP2ej8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get all representations for a single topic, we simply run the following:"
      ],
      "metadata": {
        "id": "hUYo2tokvhCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(1, full=True)"
      ],
      "metadata": {
        "id": "CmfKtFIcvkrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic, values in topic_model.topic_aspects_[\"KeyBERT\"].items():\n",
        "  if topic == 1:\n",
        "    print(topic, \":\", values)"
      ],
      "metadata": {
        "id": "ntEWigDOluFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic, values in topic_model.topic_aspects_[\"KeyBERT\"].items():\n",
        "  if topic == 1:\n",
        "    print(topic, \":\", \" | \".join(list(zip(*values))[0][:3]))"
      ],
      "metadata": {
        "id": "ndLJJEeimqVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label the topics yourself\n",
        "\n",
        "\n",
        "# or use one of the other topic representations, like KeyBERTInspired\n",
        "keybert_topic_labels = {topic: \" | \".join(list(zip(*values))[0][:3]) for topic, values in topic_model.topic_aspects_[\"KeyBERT\"].items()}\n",
        "topic_model.set_topic_labels(keybert_topic_labels)\n",
        "\n",
        "topic_model.set_topic_labels({1: \"Space Travel\", 7: \"Religion\"})\n",
        "\n",
        "# or ChatGPT's labels\n",
        "# chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
        "# chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
        "# topic_model.set_topic_labels(chatgpt_topic_labels)"
      ],
      "metadata": {
        "id": "meEM585mCk7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have set the updated topic labels, we can access them with the many functions used throughout BERTopic. Most notably, you can show the updated labels in visualizations with the `custom_labels=True` parameters."
      ],
      "metadata": {
        "id": "BPHq4EHayanF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "n8KZTeb6vRgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the overview in `.get_topic_info` now also includes the column `CustomName`. That is the custom label that we just created for each topic."
      ],
      "metadata": {
        "id": "DyBVZIt7y6Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic-Document Distribution**\n",
        "If using `calculate_probabilities=True` is not possible, than you can [approximate the topic-document distributions](https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html) using `.approximate_distribution`. It is a fast and flexible method for creating different topic-document distributions."
      ],
      "metadata": {
        "id": "I4GFP1c8Ex5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `topic_distr` contains the distribution of topics in each document\n",
        "topic_distr, _ = topic_model.approximate_distribution(abstracts, window=8, stride=4)"
      ],
      "metadata": {
        "id": "5Lu_XyUEEyJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, lets take a look at a specific abstract and see how the topic distribution was extracted:"
      ],
      "metadata": {
        "id": "GT0lFY0F0FHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_id = 10\n",
        "print(abstracts[abstract_id])"
      ],
      "metadata": {
        "id": "SIHHfgiEz1hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the topic-document distribution for a single document\n",
        "topic_model.visualize_distribution(topic_distr[abstract_id])"
      ],
      "metadata": {
        "id": "YgQ5b_B0FYw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the topic-document distribution for a single document\n",
        "topic_model.visualize_distribution(topic_distr[abstract_id], custom_labels=True)"
      ],
      "metadata": {
        "id": "nRP2BEbSFMPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to have extracted a number of topics that are relevant and shows the distributions of these topics across the abstract. We can go one step further and visualize them on a token-level:"
      ],
      "metadata": {
        "id": "iClmuzqP0PXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the topic distributions on a token-level\n",
        "topic_distr, topic_token_distr = topic_model.approximate_distribution(abstracts[abstract_id], calculate_tokens=True)\n",
        "\n",
        "# Visualize the token-level distributions\n",
        "df = topic_model.visualize_approximate_distribution(abstracts[abstract_id], topic_token_distr[0])\n",
        "df"
      ],
      "metadata": {
        "id": "HDM4cxYSFhPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Outlier Reduction**\n",
        "By default, HDBSCAN generates outliers which is a helpful mechanic in creating accurate topic representations. However, you might want to assign every single document to a topic. We can use `.reduce_outliers` to map some or all outliers to a topic:"
      ],
      "metadata": {
        "id": "6Zegs-RlGLIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce outliers\n",
        "new_topics = topic_model.reduce_outliers(abstracts, topics)\n",
        "\n",
        "# Reduce outliers with pre-calculate embeddings instead\n",
        "new_topics = topic_model.reduce_outliers(abstracts, topics, strategy=\"embeddings\", embeddings=embeddings)"
      ],
      "metadata": {
        "id": "nWhlaE3OF-9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.update_topics(abstracts, topics=new_topics)"
      ],
      "metadata": {
        "id": "l84X2L4cqWWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "46-Tpll-qXEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ’¡  NOTE - Update Topics with Outlier Reduction ðŸ’¡**\n",
        "***\n",
        "After having generated updated topic assignments, we can pass them to BERTopic in order to update the topic representations:\n",
        "\n",
        "```python\n",
        "topic_model.update_topics(docs, topics=new_topics)\n",
        "```\n",
        "\n",
        "It is important to realize that updating the topics this way may lead to errors if topic reduction or topic merging techniques are used afterwards. The reason for this is that when you assign a -1 document to topic 1 and another -1 document to topic 2, it is unclear how you map the -1 documents. Is it matched to topic 1 or 2.\n",
        "***"
      ],
      "metadata": {
        "id": "xjmXkGQZGZKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualize Topics**\n",
        "\n",
        "With visualizations, we are closing into the realm of subjective \"best practices\". These are things that I generally do because I like the representations but your experience might differ.\n",
        "\n",
        "Having said that, there are two visualizations that are my go-to when visualizing the topics themselves:\n",
        "\n",
        "* `topic_model.visualize_topics()`\n",
        "* `topic_model.visualize_hierarchy()`"
      ],
      "metadata": {
        "id": "7moKs4b5ANUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics(custom_labels=True)"
      ],
      "metadata": {
        "id": "q9kRc7MD-XMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_hierarchy(custom_labels=True)"
      ],
      "metadata": {
        "id": "JRgjSV_sFQtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualize Documents**\n",
        "\n",
        "When visualizing documents, it helps to have embedded the documents beforehand to speed up computation. Fortunately, we have already done that as a \"best practice\".\n",
        "\n",
        "Visualizing documents in 2-dimensional space helps in understanding the underlying structure of the documents and topics."
      ],
      "metadata": {
        "id": "TBEtZ_P7A0ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "wsYYwd2pDLBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following plot is **interactive** which means that you can zoom in, double click on a label to only see that one and generally interact with the plot:"
      ],
      "metadata": {
        "id": "u03NIcZdGGC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the documents in 2-dimensional space and show the titles on hover instead of the abstracts\n",
        "# NOTE: You can hide the hover with `hide_document_hover=True` which is especially helpful if you have a large dataset\n",
        "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True)"
      ],
      "metadata": {
        "id": "raUhiB9YBCCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also hide the annotation to have a more clear overview of the topics\n",
        "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)"
      ],
      "metadata": {
        "id": "91_cyh9MGOaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ’¡  NOTE - 2-dimensional space ðŸ’¡**\n",
        "***\n",
        "Although visualizing the documents in 2-dimensional gives an idea of their underlying structure, there is a risk involved.\n",
        "\n",
        "Visualizing the documents in 2-dimensional space means that we have lost significant information since the original embeddings were more than 384 dimensions. Condensing all that information in 2 dimensions is simply not possible. In other words, it is merely an **approximation**, albeit quite an accurate one.\n",
        "***"
      ],
      "metadata": {
        "id": "OVhoKO4ABEuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Serialization**\n",
        "\n",
        "When saving a BERTopic model, there are several ways in doing so. You can either save the entire model with `pickle`, `pytorch`, or `safetensors`.\n",
        "\n",
        "Personally, I would advise going with `safetensors` whenever possible. The reason for this is that the format allows for a very small topic model to be saved and shared.\n",
        "\n",
        "When saving a model with `safetensors`, it skips over saving the dimensionality reduction and clustering models. The `.transform` function will still work without these models but instead assign topics based on the similarity between document embeddings and the topic embeddings.\n",
        "\n",
        "As a result, the `.transform` step might give different results but it is generally worth it considering the smaller and significantly faster model."
      ],
      "metadata": {
        "id": "VVk0FPLu7v2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "topic_model.save(\"my_model_dir\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)"
      ],
      "metadata": {
        "id": "9Jx2uQDK8fHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ’¡  NOTE - Embedding Model ðŸ’¡**\n",
        "***\n",
        "Using `safetensors`, we are not saving the underlying embedding model but merely a pointer to the model. For example, in the above example we are saving the string `\"sentence-transformers/all-MiniLM-L6-v2\"` so that we can load in the embedding model alongside the topic model.\n",
        "\n",
        "This currently only works if you are using a sentence transformer model. If you are using a different model, you can load it in when loading the topic model like this:\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load model and add embedding model\n",
        "loaded_model = BERTopic.load(\"path/to/my/model_dir\", embedding_model=embedding_model)\n",
        "```\n",
        "***"
      ],
      "metadata": {
        "id": "qrRmrIDp8jLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above, loading can be done as follows:"
      ],
      "metadata": {
        "id": "bvdYM4AA4Pjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load model and add embedding model\n",
        "loaded_model = BERTopic.load(\"my_model_dir\", embedding_model=embedding_model)"
      ],
      "metadata": {
        "id": "QUH8Jhbs4RuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**\n",
        "\n",
        "To speed up the inference, we can leverage a \"best practice\" that we used before, namely serialization. When you save a model as `safetensors` and then load it in, we are removing the dimensionality reduction and clustering steps from the pipeline.\n",
        "\n",
        "Instead, the assignment of topics is done through cosine similarity of document embeddings and topic embeddings. This speeds up inferences significantly.\n",
        "\n",
        "To show its effect, let's start by disabling the logger:"
      ],
      "metadata": {
        "id": "m3aN-f9B4rmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic._utils import MyLogger\n",
        "logger = MyLogger(\"ERROR\")\n",
        "loaded_model.verbose = False\n",
        "topic_model.verbose = False"
      ],
      "metadata": {
        "id": "QF0tIuMy5qSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we run inference on both the loaded model and the non-loaded model:"
      ],
      "metadata": {
        "id": "PbfYD4Sq53z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit loaded_model.transform(abstracts[:100])"
      ],
      "metadata": {
        "id": "c49MiOj24WGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit topic_model.transform(abstracts[:100])"
      ],
      "metadata": {
        "id": "lMlk3lkR5Ofd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1000 documents**"
      ],
      "metadata": {
        "id": "sGkChwWj6JLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit loaded_model.transform(abstracts[:1000])"
      ],
      "metadata": {
        "id": "KcmGXdP056t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit topic_model.transform(abstracts[:1000])"
      ],
      "metadata": {
        "id": "ZL6RdmAx58IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10_000 documents**"
      ],
      "metadata": {
        "id": "LgH0Ut1j6SsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit loaded_model.transform(abstracts[:10000])"
      ],
      "metadata": {
        "id": "Ys-50O1a6OVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit topic_model.transform(abstracts[:10000])"
      ],
      "metadata": {
        "id": "pJm7KYU76QVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above, the `loaded_model` seems to be quite a bit faster for inference than the original `topic_model`."
      ],
      "metadata": {
        "id": "KBWDH0x35XkX"
      }
    }
  ]
}